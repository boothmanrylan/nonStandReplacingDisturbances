{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMv6tTPgaKICI/nJH+RUS65",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boothmanrylan/nonStandReplacingDisturbances/blob/colab_dev/nonStandReplacingDisturbances.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/boothmanrylan/nonStandReplacingDisturbances.git\n",
        "%cd nonStandReplacingDisturbances"
      ],
      "metadata": {
        "id": "yUL8fDtjqqna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBMd-B1Ipxke"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import google\n",
        "from google.colab import auth\n",
        "import ee\n",
        "import geemap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "\n",
        "project = 'api-project-269347469410'\n",
        "asset_path = f\"projects/{project}/assets/rylan-nonstandreplacingdisturbances\"\n",
        "\n",
        "os.environ['GOOGLE_CLOUD_PROJECT'] = project\n",
        "!gcloud config set project {project}\n",
        "\n",
        "credentials, _ = google.auth.default()\n",
        "ee.Initialize(\n",
        "    credentials,\n",
        "    project=project,\n",
        "    # opt_url='https://earthengine-highvolume.googleapis.com',\n",
        ")"
      ],
      "metadata": {
        "id": "PVgiQAoQqBvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_POINTS = 500  # points per train/test/val group\n",
        "\n",
        "disturbed_regions = ee.FeatureCollection(f\"{asset_path}/my-data/usfs-nsr-disturbances\")\n",
        "buffered_disturbed_regions = disturbed_regions.map(\n",
        "    lambda x: x.buffer(500, 100).bounds(100)\n",
        ")\n",
        "buffered_geometry = buffered_disturbed_regions.geometry(100).dissolve(100)\n",
        "\n",
        "def split_multipolygon(multipolygon):\n",
        "    # based on: https://gis.stackexchange.com/a/444779\n",
        "    size = multipolygon.coordinates().size()\n",
        "    indices = ee.List.sequence(0, size.subtract(1))\n",
        "\n",
        "    def grab_polygon(i):\n",
        "        geom = ee.Geometry.Polygon(multipolygon.coordinates().get(i))\n",
        "        return ee.Feature(geom, {'id': i, 'area': geom.area(100)})\n",
        "\n",
        "    return ee.FeatureCollection(indices.map(grab_polygon))\n",
        "\n",
        "split_geometry = split_multipolygon(buffered_geometry)\n",
        "\n",
        "# split into approx. 1/3 area to each of train/test/val by sorting by area and\n",
        "# then extracting every third geometry\n",
        "split_geometry = split_geometry.sort('area', False)\n",
        "N = split_geometry.size().subtract(1)\n",
        "\n",
        "train_indices = ee.List.sequence(0, N, 3)\n",
        "test_indices = ee.List.sequence(1, N, 3)\n",
        "val_indices = ee.List.sequence(2, N, 3)\n",
        "\n",
        "train_regions = split_geometry.filter(ee.Filter.inList('id', train_indices))\n",
        "test_regions = split_geometry.filter(ee.Filter.inList('id', test_indices))\n",
        "val_regions = split_geometry.filter(ee.Filter.inList('id', val_indices))\n",
        "\n",
        "def sample_points(rois):\n",
        "    disturbed_polys = disturbed_regions.filterBounds(rois)\n",
        "\n",
        "    # ensure each polygon has at least one samples in it\n",
        "    specific_disturbed_points = ee.FeatureCollection(disturbed_polys.map(\n",
        "        lambda x: ee.FeatureCollection.randomPoints(x.geometry(), 3, 42)\n",
        "    )).flatten()\n",
        "\n",
        "    # ensure that larger polygons have more than two samples in them\n",
        "    N = specific_disturbed_points.size()\n",
        "    print(N.getInfo())\n",
        "    other_disturbed_points = ee.FeatureCollection.randomPoints(\n",
        "        disturbed_polys.geometry(),\n",
        "        N.divide(10).int(),\n",
        "        42\n",
        "    )\n",
        "    disturbed_points = specific_disturbed_points.merge(other_disturbed_points)\n",
        "    disturbed_points = disturbed_points.map(lambda x: x.set('class', 1))\n",
        "\n",
        "    # ensure that there is the same number of disturbed as undisturbed samples\n",
        "    undisturbed_points = ee.FeatureCollection.randomPoints(\n",
        "        rois.geometry().difference(disturbed_polys),\n",
        "        disturbed_points.size(),\n",
        "        42,\n",
        "    ).map(lambda x: x.set('class', 0))\n",
        "\n",
        "    return disturbed_points.merge(undisturbed_points)\n",
        "\n",
        "train_points = sample_points(train_regions)\n",
        "test_points = sample_points(test_regions)\n",
        "val_points = sample_points(val_regions)\n",
        "\n",
        "# disturbance_mask = disturbed_regions.map(\n",
        "#     lambda x: x.set('foo', 1)\n",
        "# ).reducetoimage(\n",
        "#     ['foo'], ee.reducer.first()\n",
        "# ).unmask().gt(0)\n",
        "print(train_points.size().getInfo(), test_points.size().getInfo(), val_points.size().getInfo())"
      ],
      "metadata": {
        "id": "o9MJs8AnqWpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map = geemap.Map()\n",
        "Map.addLayer(disturbed_regions, {}, 'Disturbed Regions')\n",
        "Map.addLayer(split_geometry, {'color': 'white'}, 'ROI')\n",
        "Map.addLayer(train_points, {'color': 'red'}, 'Train Points')\n",
        "Map.addLayer(test_points, {'color': 'blue'}, 'Test Points')\n",
        "Map.addLayer(val_points, {'color': 'yellow'}, 'Val Points')\n",
        "Map"
      ],
      "metadata": {
        "id": "zpVLbxdzsXJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial_export_region = ee.Geometry.Rectangle([[-120.49118, 40.033924], [-120.29068, 40.208246]])\n",
        "trial_export_points = train_points.filterBounds(trial_export_region).randomColumn('random', 42).limit(50, 'random')"
      ],
      "metadata": {
        "id": "TtiEfTlXMMa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from multiprocessing import Pool\n",
        "\n",
        "from goolge.api_core import retry\n",
        "import webdataset as wbs\n",
        "import torch\n",
        "\n",
        "def create_time_chunk_list(start, end, delta):\n",
        "    start = ee.Date(start).millis()\n",
        "    end = ee.Date(end).millis()\n",
        "    starts = ee.List.sequence(start, end, delta)\n",
        "    ends = starts.map(lambda x: x.add(delta))\n",
        "    return starts.zip(ends)\n",
        "\n",
        "\n",
        "def get_s1(point, start, end, delta):\n",
        "    \"\"\" Creats a median Sentinel1 image for each delta between start and end.\n",
        "\n",
        "    Based on:\n",
        "    openmapflow/openmapflow/eo/sentinel1.py:get_image_collection\n",
        "    and\n",
        "    openmapflow/openmapflow/eo/sentinel1.py:get_single_image\n",
        "    from: https://github.com/nasaharvest/openmapflow/tree/main\n",
        "\n",
        "    Args:\n",
        "        point: ee.Geometry, used to filterBounds of the complete sentinel1\n",
        "        start: string, start date in format YYYY-MM-dd\n",
        "        end: string, end date in format YYYY-MM-dd\n",
        "        delta: int, chunk length to split total time period into (in ms)\n",
        "\n",
        "    Returns:\n",
        "        ee.Image\n",
        "    \"\"\"\n",
        "    col = (\n",
        "        ee.ImageCollection(\"COPERNICUS/S1_GRD\")\n",
        "        .filterDate(start, end)\n",
        "        .filterBounds(point)\n",
        "        .filter(ee.Filter.eq(\"instrumentMode\", \"IW\"))\n",
        "        .filter(ee.Filter.listContains(\"transmitterReceiverPolarisation\", \"VV\"))\n",
        "        .filter(ee.Filter.listContains(\"transmitterReceiverPolarisation\", \"VH\"))\n",
        "    )\n",
        "\n",
        "    # want all images to either be descending or ascending, but not both. also\n",
        "    # want as many observations as possible to take the collection that has the\n",
        "    # most images in it\n",
        "    descend_col = col.filter(ee.Filter.eq(\"orbitProperties_pass\", \"DESCENDING\"))\n",
        "    ascend_col = col.filter(ee.filter.eq(\"orbitProperties_pass\", \"ASCENDING\"))\n",
        "    col = ee.Algorithms.If(\n",
        "        descend_col.size().gt(ascend_col.size()),\n",
        "        descend_col,\n",
        "        ascend_col,\n",
        "    )\n",
        "\n",
        "    def process_time_chunk(chunk):\n",
        "        chunk = ee.List(chunk)\n",
        "        curr_chunk = col.filterDate(chunk.get(0), chunk.get(1))\n",
        "        return curr_chunk.median().select([\"VV\", \"VH\"])\n",
        "\n",
        "    time_chunks = create_time_chunk_list(start, end, delta)\n",
        "\n",
        "    return ee.ImageCollection(time_chunks.map(process_time_chunk)).toBands()\n",
        "\n",
        "\n",
        "def get_s2(point, start, end, delta):\n",
        "    \"\"\" Gets a quality mosaic Sentinel2 image for each delta between start and end.\n",
        "\n",
        "    Based on:\n",
        "    openmapflow/openmapflow/eo/sentinel1.py:get_single_image\n",
        "    from: https://github.com/nasaharvest/openmapflow/tree/main\n",
        "\n",
        "    Args:\n",
        "        point: ee.Geometry, used to filterBounds\n",
        "        start: string, start date in format YYYY-MM-dd\n",
        "        end: string, end date in format YYYY-MM-dd\n",
        "\n",
        "    Returns:\n",
        "        ee.Image\n",
        "    \"\"\"\n",
        "    col = (\n",
        "        ee.ImageCollection(\"COPERNICUS/S2\")\n",
        "        .filterDate(start, end)\n",
        "        .filterBounds(point)\n",
        "    )\n",
        "\n",
        "    def process_time_chunk(chunk):\n",
        "        chunk = ee.List(chunk)\n",
        "        cloud_score_plus = ee.ImageCollection(\"GOOGLE/CLOUD_SCORE_PLUS/V1/S2_HARMONIZED\")\n",
        "        curr_chunk = (\n",
        "            col.filterDate(chunk.get(0), chunk.get(1))\n",
        "            .linkCollection(cloud_score_plust, [\"cs_cdf\"])\n",
        "        )\n",
        "        # TODO: is a qualityMosaic better than masking and taking the median?\n",
        "        return curr_chunk.qualityMosaic(\"cs_cdf\")\n",
        "\n",
        "    time_chunks = create_time_chunk_list(start, end, delta)\n",
        "\n",
        "    return ee.ImageCollection(time_chunks.map(process_time_chunk)).toBands()\n",
        "\n",
        "\n",
        "def get_era5(point, start, end, delta):\n",
        "    \"\"\" Gets an ERA5 image for each delta between start and end.\n",
        "\n",
        "    All ERA5 images are dated to the first of the month, we choose to use\n",
        "    the ERA5 image based on the start date of each delta period.\n",
        "\n",
        "    Based on:\n",
        "    openmapflow/openmapflow/eo/era5.py:get_single_image\n",
        "    from: https://github.com/nasaharvest/openmapflow/tree/main\n",
        "\n",
        "    Args:\n",
        "        point: ee.Geometry, used to filterBounds\n",
        "        start: string, start date in format YYYY-MM-dd\n",
        "        end: string, end date in format YYYY-MM-dd\n",
        "\n",
        "    Returns:\n",
        "        ee.Image\n",
        "    \"\"\"\n",
        "    col = (\n",
        "        ee.ImageCollection(\"ECMWF/ERA5_LAND/MONTHLY_AGGR\")\n",
        "        .filterDate(start, end)\n",
        "        .filterBounds(point)\n",
        "    )\n",
        "\n",
        "    def process_time_chunk(chunk):\n",
        "        start = ee.List(chunk).get(0)\n",
        "        start_year = chunk.get(\"year\")\n",
        "        start_month = chunk.get(\"month\")\n",
        "        start_date = ee.Date.fromYMD(start_year, start_month, 1)\n",
        "        curr_chunk = col.filterDate(\n",
        "            start_date.advance(-1, \"day\"),\n",
        "            start_date.advance(1, \"day\"),\n",
        "        )\n",
        "        return curr_chunk.mean()\n",
        "\n",
        "    time_chunks = create_time_chunk_list(start, end, delta)\n",
        "\n",
        "    return ee.ImageCollection(time_chunks.map(process_time_chunk)).toBands()\n",
        "\n",
        "\n",
        "def get_srtm(*args, **kwargs):\n",
        "    \"\"\" Gets the SRTM DEM with calculated slope.\n",
        "\n",
        "    The SRTM is a single image that covers the entire globe at a single point in\n",
        "    time, therefore no need to create an image for each time step.\n",
        "\n",
        "    Args:\n",
        "        *args, **kwargs to allow for consistent usage with other data getters\n",
        "\n",
        "    Returns:\n",
        "        ee.Image\n",
        "    \"\"\"\n",
        "    elevation = ee.Image(\"USGS/SRTMGL1_003\").rename(\"elevation\")\n",
        "    slope = ee.Terrain.slope(elevation).rename(\"slope\")\n",
        "    return elevation.addBands(slope)\n",
        "\n",
        "\n",
        "def get_dynamic_world(point, start, end, delta):\n",
        "    \"\"\" Gets a mode Dynamic World image for each delta between start and stop.\n",
        "\n",
        "    Based on:\n",
        "    presto/presto/dataops/pipelines/dynamicworld.py:DynamicWorldMonthly2020_2021\n",
        "    from: https://github.com/nasaharvest/presto\n",
        "\n",
        "    Args:\n",
        "        point: ee.Geometry, used to filterBounds\n",
        "        start: string, start date in format YYYY-MM-dd\n",
        "        end: string, end date in format YYYY-MM-dd\n",
        "\n",
        "    Returns:\n",
        "        ee.Image\n",
        "    \"\"\"\n",
        "    col = (\n",
        "        ee.ImageCollection(\"GOOGLE/DYNAMICWORLD/V1\")\n",
        "        .filterBounds(point)\n",
        "        .filterDate(start, end)\n",
        "    )\n",
        "\n",
        "    def process_time_chunk(chunk):\n",
        "        chunk = ee.List(chunk)\n",
        "        curr_chunk = col.filterDate(chunk.get(0), chunk.get(1))\n",
        "        return curr_chunk.mode()\n",
        "\n",
        "    time_chunks = create_time_chunk_list(start, end, delta)\n",
        "\n",
        "    return ee.ImageCollection(time_chunks.map(process_time_chunk)).toBands()\n",
        "\n",
        "\n",
        "@retry.Retry()\n",
        "def process_point_fn(point, start, end, delta, projection=PROJECTION, scale=SCALE):\n",
        "    point = ee.Feature(point)\n",
        "\n",
        "    # project the point\n",
        "    coords = point.geometry(1, projection).getInfo()[\"coordinates\"]\n",
        "\n",
        "    request = {\n",
        "        \"fileFormat\": \"NUMPY_NDARRAY\",\n",
        "        \"grid\": {\n",
        "            \"dimensions\": {\n",
        "                \"width\": 1,\n",
        "                \"height\": 1,\n",
        "            },\n",
        "            \"affineTransform\": {\n",
        "                \"scaleX\": scale,\n",
        "                \"shearX\": 0,\n",
        "                \"translateX\": coords[0],\n",
        "                \"shearY\": 0,\n",
        "                \"scaleY\": -scale,\n",
        "                \"translateY\": coords[1],\n",
        "            },\n",
        "            \"crsCode\": crs\n",
        "        }\n",
        "    }\n",
        "\n",
        "    data = {}\n",
        "    bands = {}\n",
        "    data_getters = {\n",
        "        \"s1\": get_s1,\n",
        "        \"s2\": get_s2,\n",
        "        \"era5\": get_era5,\n",
        "        \"srtm\": get_srtm,\n",
        "        \"dynamic_world\": get_dynamic_world,\n",
        "    }\n",
        "    for source, getter in data_gettters.items():\n",
        "        image = getter(point, start, end, delta)\n",
        "        request[\"expression\"] = image\n",
        "        data[source] = torch.as_tensor(ee.data.computePixels(request))\n",
        "        if source != \"dynamic_world\":\n",
        "            bands[f\"{source}_bands\"] = image.bandNames().getInfo()\n",
        "\n",
        "    presto_input = construct_single_presto_input(**data, **bands)\n",
        "    months = create_month_tensor(start, end, delta)\n",
        "    class_label = point.get(\"class\")\n",
        "\n",
        "    # TODO also need to save the dates (month) of each input\n",
        "    # TODO also need to save the latitude/longitude of the point\n",
        "\n",
        "    # TODO: save the presto input using webdataset\n",
        "\n",
        "with Pool(MAX_PARALLEL_TASKS) as p:\n",
        "    p.map(process_point_fn, points_list)"
      ],
      "metadata": {
        "id": "V3ybeNHGYHTb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}