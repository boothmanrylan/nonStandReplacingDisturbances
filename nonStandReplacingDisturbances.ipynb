{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMIu4G3PhNSHv/MF3MhRiRF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boothmanrylan/nonStandReplacingDisturbances/blob/colab_dev/nonStandReplacingDisturbances.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/boothmanrylan/presto.git\n",
        "%cd presto\n",
        "\n",
        "# replace all install_requires '==' with '>=' to make installing to colab env easier\n",
        "setup_text = []\n",
        "with open(\"setup.py\", 'r') as f:\n",
        "    for line in f.readlines():\n",
        "        setup_text.append(line.replace(\"==\", \">=\"))\n",
        "\n",
        "with open(\"setup.py\", \"w\") as f:\n",
        "    for line in setup_text:\n",
        "        f.write(line)\n",
        "\n",
        "!pip install -e .\n",
        "%cd .."
      ],
      "metadata": {
        "id": "WJBh-pbRPPTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/boothmanrylan/nonStandReplacingDisturbances.git\n",
        "# %cd nonStandReplacingDisturbances"
      ],
      "metadata": {
        "id": "yUL8fDtjqqna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBMd-B1Ipxke"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import google\n",
        "from google.colab import auth\n",
        "import ee\n",
        "import geemap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "\n",
        "project = 'api-project-269347469410'\n",
        "asset_path = f\"projects/{project}/assets/rylan-nonstandreplacingdisturbances\"\n",
        "\n",
        "os.environ['GOOGLE_CLOUD_PROJECT'] = project\n",
        "!gcloud config set project {project}\n",
        "\n",
        "credentials, _ = google.auth.default()\n",
        "ee.Initialize(\n",
        "    credentials,\n",
        "    project=project,\n",
        "    opt_url='https://earthengine-highvolume.googleapis.com',\n",
        ")"
      ],
      "metadata": {
        "id": "PVgiQAoQqBvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_POINTS = 500  # points per train/test/val group\n",
        "\n",
        "disturbed_regions = ee.FeatureCollection(f\"{asset_path}/my-data/usfs-nsr-disturbances\")\n",
        "buffered_disturbed_regions = disturbed_regions.map(\n",
        "    lambda x: x.buffer(500, 100).bounds(100)\n",
        ")\n",
        "buffered_geometry = buffered_disturbed_regions.geometry(100).dissolve(100)\n",
        "\n",
        "def split_multipolygon(multipolygon):\n",
        "    # based on: https://gis.stackexchange.com/a/444779\n",
        "    size = multipolygon.coordinates().size()\n",
        "    indices = ee.List.sequence(0, size.subtract(1))\n",
        "\n",
        "    def grab_polygon(i):\n",
        "        geom = ee.Geometry.Polygon(multipolygon.coordinates().get(i))\n",
        "        return ee.Feature(geom, {'id': i, 'area': geom.area(100)})\n",
        "\n",
        "    return ee.FeatureCollection(indices.map(grab_polygon))\n",
        "\n",
        "split_geometry = split_multipolygon(buffered_geometry)\n",
        "\n",
        "# split into approx. 1/3 area to each of train/test/val by sorting by area and\n",
        "# then extracting every third geometry\n",
        "split_geometry = split_geometry.sort('area', False)\n",
        "N = split_geometry.size().subtract(1)\n",
        "\n",
        "train_indices = ee.List.sequence(0, N, 3)\n",
        "test_indices = ee.List.sequence(1, N, 3)\n",
        "val_indices = ee.List.sequence(2, N, 3)\n",
        "\n",
        "train_regions = split_geometry.filter(ee.Filter.inList('id', train_indices))\n",
        "test_regions = split_geometry.filter(ee.Filter.inList('id', test_indices))\n",
        "val_regions = split_geometry.filter(ee.Filter.inList('id', val_indices))\n",
        "\n",
        "def sample_points(rois):\n",
        "    disturbed_polys = disturbed_regions.filterBounds(rois)\n",
        "\n",
        "    # ensure each polygon has at least one samples in it\n",
        "    specific_disturbed_points = ee.FeatureCollection(disturbed_polys.map(\n",
        "        lambda x: ee.FeatureCollection.randomPoints(x.geometry(), 3, 42)\n",
        "    )).flatten()\n",
        "\n",
        "    # ensure that larger polygons have more than two samples in them\n",
        "    N = specific_disturbed_points.size()\n",
        "    print(N.getInfo())\n",
        "    other_disturbed_points = ee.FeatureCollection.randomPoints(\n",
        "        disturbed_polys.geometry(),\n",
        "        N.divide(10).int(),\n",
        "        42\n",
        "    )\n",
        "    disturbed_points = specific_disturbed_points.merge(other_disturbed_points)\n",
        "    disturbed_points = disturbed_points.map(lambda x: x.set('class', 1))\n",
        "\n",
        "    # ensure that there is the same number of disturbed as undisturbed samples\n",
        "    undisturbed_points = ee.FeatureCollection.randomPoints(\n",
        "        rois.geometry().difference(disturbed_polys),\n",
        "        disturbed_points.size(),\n",
        "        42,\n",
        "    ).map(lambda x: x.set('class', 0))\n",
        "\n",
        "    return disturbed_points.merge(undisturbed_points)\n",
        "\n",
        "train_points = sample_points(train_regions)\n",
        "test_points = sample_points(test_regions)\n",
        "val_points = sample_points(val_regions)\n",
        "\n",
        "# disturbance_mask = disturbed_regions.map(\n",
        "#     lambda x: x.set('foo', 1)\n",
        "# ).reducetoimage(\n",
        "#     ['foo'], ee.reducer.first()\n",
        "# ).unmask().gt(0)\n",
        "print(train_points.size().getInfo(), test_points.size().getInfo(), val_points.size().getInfo())"
      ],
      "metadata": {
        "id": "o9MJs8AnqWpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Map = geemap.Map()\n",
        "Map.addLayer(disturbed_regions, {}, 'Disturbed Regions')\n",
        "Map.addLayer(split_geometry, {'color': 'white'}, 'ROI')\n",
        "Map.addLayer(train_points, {'color': 'red'}, 'Train Points')\n",
        "Map.addLayer(test_points, {'color': 'blue'}, 'Test Points')\n",
        "Map.addLayer(val_points, {'color': 'yellow'}, 'Val Points')\n",
        "Map"
      ],
      "metadata": {
        "id": "zpVLbxdzsXJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial_export_region = ee.Geometry.Rectangle([[-120.49118, 40.033924], [-120.29068, 40.208246]])\n",
        "trial_export_points = train_points.filterBounds(trial_export_region).randomColumn('random', 42).limit(50, 'random')"
      ],
      "metadata": {
        "id": "TtiEfTlXMMa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from multiprocessing import Pool\n",
        "\n",
        "import numpy as np\n",
        "from numpy.lib.recfunctions import structured_to_unstructured\n",
        "from google.api_core import retry\n",
        "import webdataset as wds\n",
        "import torch\n",
        "from presto import presto\n",
        "\n",
        "SCALE = 10\n",
        "CRS = \"EPSG:3857\"\n",
        "PROJECTION = ee.Projection(CRS)\n",
        "MILLIS_PER_MONTH = 30 * 24 * 60 * 60 * 1000\n",
        "MAX_PARALLEL_TASKS = 40\n",
        "S2_BANDS = [\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\",\n",
        "            \"B8\", \"B8A\", \"B10\", \"B11\", \"B12\"]\n",
        "S1_BANDS = [\"VV\", \"VH\"]\n",
        "ERA5_BANDS = [\"temperature_2m\", \"total_precipitation\"]\n",
        "SRTM_BANDS = [\"elevation\", \"slope\"]\n",
        "DYNAMIC_WORLD_BANDS = [\"label\"]\n",
        "\n",
        "# TODO: do we want to exclude winter months (or allow for winter months to be excluded)\n",
        "def create_time_chunk_list(start, end, delta):\n",
        "    start = ee.Date(start).millis()\n",
        "    end = ee.Date(end).millis()\n",
        "    starts = ee.List.sequence(start, end, delta)\n",
        "    ends = starts.map(lambda x: ee.Number(x).add(delta))\n",
        "    return starts.zip(ends)\n",
        "\n",
        "\n",
        "def get_s1(point, start, end, delta):\n",
        "    \"\"\" Creats a median Sentinel1 image for each delta between start and end.\n",
        "\n",
        "    Based on:\n",
        "    openmapflow/openmapflow/eo/sentinel1.py:get_image_collection\n",
        "    and\n",
        "    openmapflow/openmapflow/eo/sentinel1.py:get_single_image\n",
        "    from: https://github.com/nasaharvest/openmapflow/tree/main\n",
        "\n",
        "    Args:\n",
        "        point: ee.Geometry, used to filterBounds of the complete sentinel1\n",
        "        start: string, start date in format YYYY-MM-dd\n",
        "        end: string, end date in format YYYY-MM-dd\n",
        "        delta: int, chunk length to split total time period into (in ms)\n",
        "\n",
        "    Returns:\n",
        "        ee.Image\n",
        "    \"\"\"\n",
        "    col = (\n",
        "        ee.ImageCollection(\"COPERNICUS/S1_GRD\")\n",
        "        .filterDate(start, end)\n",
        "        .filterBounds(point)\n",
        "        .filter(ee.Filter.eq(\"instrumentMode\", \"IW\"))\n",
        "        .filter(ee.Filter.listContains(\"transmitterReceiverPolarisation\", \"VV\"))\n",
        "        .filter(ee.Filter.listContains(\"transmitterReceiverPolarisation\", \"VH\"))\n",
        "    )\n",
        "\n",
        "    # want all images to either be descending or ascending, but not both. also\n",
        "    # want as many observations as possible to take the collection that has the\n",
        "    # most images in it\n",
        "    descend_col = col.filter(ee.Filter.eq(\"orbitProperties_pass\", \"DESCENDING\"))\n",
        "    ascend_col = col.filter(ee.Filter.eq(\"orbitProperties_pass\", \"ASCENDING\"))\n",
        "    col = ee.ImageCollection(ee.Algorithms.If(\n",
        "        descend_col.size().gt(ascend_col.size()),\n",
        "        descend_col,\n",
        "        ascend_col,\n",
        "    ))\n",
        "\n",
        "    def process_time_chunk(chunk):\n",
        "        chunk = ee.List(chunk)\n",
        "        curr_chunk = col.filterDate(chunk.get(0), chunk.get(1))\n",
        "        curr_chunk = curr_chunk.median().select([\"VV\", \"VH\"])\n",
        "        return curr_chunk.toLong()\n",
        "\n",
        "    time_chunks = create_time_chunk_list(start, end, delta)\n",
        "\n",
        "    return ee.ImageCollection(time_chunks.map(process_time_chunk)).toBands(), S1_BANDS\n",
        "\n",
        "\n",
        "def get_s2(point, start, end, delta):\n",
        "    \"\"\" Gets a quality mosaic Sentinel2 image for each delta between start and end.\n",
        "\n",
        "    Based on:\n",
        "    openmapflow/openmapflow/eo/sentinel1.py:get_single_image\n",
        "    from: https://github.com/nasaharvest/openmapflow/tree/main\n",
        "\n",
        "    Args:\n",
        "        point: ee.Geometry, used to filterBounds\n",
        "        start: string, start date in format YYYY-MM-dd\n",
        "        end: string, end date in format YYYY-MM-dd\n",
        "\n",
        "    Returns:\n",
        "        ee.Image\n",
        "    \"\"\"\n",
        "    col = (\n",
        "        ee.ImageCollection(\"COPERNICUS/S2\")\n",
        "        .filterDate(start, end)\n",
        "        .filterBounds(point)\n",
        "    )\n",
        "\n",
        "    def process_time_chunk(chunk):\n",
        "        chunk = ee.List(chunk)\n",
        "        cloud_score_plus = ee.ImageCollection(\"GOOGLE/CLOUD_SCORE_PLUS/V1/S2_HARMONIZED\")\n",
        "        curr_chunk = (\n",
        "            col.filterDate(chunk.get(0), chunk.get(1))\n",
        "            .linkCollection(cloud_score_plus, [\"cs_cdf\"])\n",
        "        )\n",
        "        # TODO: is a qualityMosaic better than masking and taking the median?\n",
        "        return curr_chunk.qualityMosaic(\"cs_cdf\").select(S2_BANDS).toLong()\n",
        "\n",
        "    time_chunks = create_time_chunk_list(start, end, delta)\n",
        "\n",
        "    return ee.ImageCollection(time_chunks.map(process_time_chunk)).toBands(), S2_BANDS\n",
        "\n",
        "\n",
        "def get_era5(point, start, end, delta):\n",
        "    \"\"\" Gets an ERA5 image for each delta between start and end.\n",
        "\n",
        "    All ERA5 images are dated to the first of the month, we choose to use\n",
        "    the ERA5 image based on the start date of each delta period.\n",
        "\n",
        "    Based on:\n",
        "    openmapflow/openmapflow/eo/era5.py:get_single_image\n",
        "    from: https://github.com/nasaharvest/openmapflow/tree/main\n",
        "\n",
        "    Args:\n",
        "        point: ee.Geometry, used to filterBounds\n",
        "        start: string, start date in format YYYY-MM-dd\n",
        "        end: string, end date in format YYYY-MM-dd\n",
        "\n",
        "    Returns:\n",
        "        ee.Image\n",
        "    \"\"\"\n",
        "    col = (\n",
        "        ee.ImageCollection(\"ECMWF/ERA5_LAND/MONTHLY_AGGR\")\n",
        "        .filterDate(start, end)\n",
        "        .filterBounds(point)\n",
        "    )\n",
        "\n",
        "    def process_time_chunk(chunk):\n",
        "        start = ee.Date(ee.List(chunk).get(0))\n",
        "        start_year = start.get(\"year\")\n",
        "        start_month = start.get(\"month\")\n",
        "        start_date = ee.Date.fromYMD(start_year, start_month, 1)\n",
        "        curr_chunk = col.filterDate(\n",
        "            start_date.advance(-1, \"day\"),\n",
        "            start_date.advance(1, \"day\"),\n",
        "        ).mean()\n",
        "        temp = curr_chunk.select(\"temperature_2m\")\n",
        "        percip = curr_chunk.select(\"total_precipitation_sum\")\n",
        "        return ee.Image.cat([temp, percip]).toLong()\n",
        "\n",
        "    time_chunks = create_time_chunk_list(start, end, delta)\n",
        "\n",
        "    return ee.ImageCollection(time_chunks.map(process_time_chunk)).toBands(), ERA5_BANDS\n",
        "\n",
        "\n",
        "def get_srtm(*args, **kwargs):\n",
        "    \"\"\" Gets the SRTM DEM with calculated slope.\n",
        "\n",
        "    The SRTM is a single image that covers the entire globe at a single point in\n",
        "    time, therefore no need to create an image for each time step.\n",
        "\n",
        "    Args:\n",
        "        *args, **kwargs to allow for consistent usage with other data getters\n",
        "\n",
        "    Returns:\n",
        "        ee.Image\n",
        "    \"\"\"\n",
        "    elevation = ee.Image(\"USGS/SRTMGL1_003\").rename(\"elevation\")\n",
        "    slope = ee.Terrain.slope(elevation).rename(\"slope\")\n",
        "    return ee.Image.cat([elevation, slope]).toLong(), SRTM_BANDS\n",
        "\n",
        "\n",
        "def get_dynamic_world(point, start, end, delta):\n",
        "    \"\"\" Gets a mode Dynamic World image for each delta between start and stop.\n",
        "\n",
        "    Based on:\n",
        "    presto/presto/dataops/pipelines/dynamicworld.py:DynamicWorldMonthly2020_2021\n",
        "    from: https://github.com/nasaharvest/presto\n",
        "\n",
        "    Args:\n",
        "        point: ee.Geometry, used to filterBounds\n",
        "        start: string, start date in format YYYY-MM-dd\n",
        "        end: string, end date in format YYYY-MM-dd\n",
        "\n",
        "    Returns:\n",
        "        ee.Image\n",
        "    \"\"\"\n",
        "    col = (\n",
        "        ee.ImageCollection(\"GOOGLE/DYNAMICWORLD/V1\")\n",
        "        .filterBounds(point)\n",
        "        # .filterDate(start, end)\n",
        "    )\n",
        "\n",
        "    def process_time_chunk(chunk):\n",
        "        chunk = ee.List(chunk)\n",
        "        start = ee.Date(chunk.get(0))\n",
        "        end = ee.Date(chunk.get(1))\n",
        "\n",
        "        # in case there is no data in the current chunk replace with the mode\n",
        "        # class label from the year prior to the current chunk\n",
        "        previous_year_data = col.filterDate(start.advance(-1, \"year\"), end)\n",
        "        previous_year_mode = previous_year_data.mode().select(\"label\")\n",
        "\n",
        "        curr_chunk = (\n",
        "            col.filterDate(start, end)\n",
        "            .mode()\n",
        "            .select(\"label\")\n",
        "            .unmask(previous_year_mode)\n",
        "        )\n",
        "\n",
        "        # in case there is also no data in the previous year replace with 9\n",
        "        curr_chunk = curr_chunk.unmask(9)\n",
        "\n",
        "        return curr_chunk\n",
        "\n",
        "    time_chunks = create_time_chunk_list(start, end, delta)\n",
        "\n",
        "    return ee.ImageCollection(time_chunks.map(process_time_chunk)).toBands(), DYNAMIC_WORLD_BANDS\n",
        "\n",
        "\n",
        "def create_month_tensor(start, end, delta):\n",
        "    chunks = create_time_chunk_list(start, end, delta)\n",
        "    start_months = chunks.map(lambda x: ee.Date(ee.List(x).get(0)).get(\"month\").subtract(1))\n",
        "    return torch.as_tensor(start_months.getInfo())\n",
        "\n",
        "\n",
        "@retry.Retry()\n",
        "def process_point_fn(\n",
        "    point,\n",
        "    start=\"2023-01-01\",  # TODO: presto has an upper limit of 24 timesteps\n",
        "    end=\"2024-01-01\",\n",
        "    delta=MILLIS_PER_MONTH,\n",
        "    crs=CRS,\n",
        "    scale=SCALE,\n",
        "):\n",
        "    \"\"\" Creates Presto input for given point.\n",
        "\n",
        "    Uses ee.data.computePixels to fetch data from Earth Engine.\n",
        "\n",
        "    Can be used as the funtion in multiprocessing.Pool.map to make GEE requests\n",
        "    in parallel.\n",
        "\n",
        "    Args:\n",
        "        point: list[Number], should be the points index, the latitude of the\n",
        "            point, the longitude of the point, and the class label of the point.\n",
        "            Passing a FeatureCollection to create_points_list will convert the\n",
        "            FeatureCollection to the proper format to map this function over.\n",
        "        start: string, start date in format YYYY-MM-dd\n",
        "        end: string, end date in format YYYY-MM-dd\n",
        "        delta: int, chunk length to split total time period into (in ms).\n",
        "        crs: string, EPSG crs code defining the projection to get the data in.\n",
        "        scale: int, scale to get the data in.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    index, lat, lon, class_label = point\n",
        "    point = ee.Feature(ee.Geometry.Point((lon, lat)))\n",
        "\n",
        "    # project the point to the *unscaled* projection\n",
        "    projection = ee.Projection(crs)\n",
        "    coords = point.geometry(1, projection).getInfo()[\"coordinates\"]\n",
        "\n",
        "    request = {\n",
        "        \"fileFormat\": \"NUMPY_NDARRAY\",\n",
        "        \"grid\": {\n",
        "            \"dimensions\": {\n",
        "                \"width\": 1,\n",
        "                \"height\": 1,\n",
        "            },\n",
        "            \"affineTransform\": {\n",
        "                \"scaleX\": scale,\n",
        "                \"shearX\": 0,\n",
        "                \"translateX\": coords[0],\n",
        "                \"shearY\": 0,\n",
        "                \"scaleY\": -scale,\n",
        "                \"translateY\": coords[1],\n",
        "            },\n",
        "            \"crsCode\": crs\n",
        "        }\n",
        "    }\n",
        "\n",
        "    data = {}\n",
        "    bands = {}\n",
        "    data_getters = {\n",
        "        \"s1\": get_s1,\n",
        "        \"s2\": get_s2,\n",
        "        \"era5\": get_era5,\n",
        "        \"srtm\": get_srtm,\n",
        "        \"dynamic_world\": get_dynamic_world,\n",
        "    }\n",
        "    for source, get_image_fn in data_getters.items():\n",
        "        image, band_names = get_image_fn(point.geometry(), start, end, delta)\n",
        "        request[\"expression\"] = image\n",
        "        raw = ee.data.computePixels(request)\n",
        "        arr = structured_to_unstructured(raw).reshape(-1, len(band_names))\n",
        "        arr = arr.astype(np.float32, copy=False)\n",
        "\n",
        "        if source == \"dynamic_world\":\n",
        "            data[source] = torch.as_tensor(arr.squeeze(), dtype=torch.int)\n",
        "        else:\n",
        "            bands[f\"{source}_bands\"] = band_names\n",
        "            data[source] = torch.as_tensor(arr, dtype=torch.float32)\n",
        "\n",
        "    # repeat srtm to be the same shape as all other inputs\n",
        "    num_timesteps = data[\"dynamic_world\"].shape[0]\n",
        "    data[\"srtm\"] = data[\"srtm\"].expand(num_timesteps, -1)\n",
        "\n",
        "    x, mask, dw = presto.construct_single_presto_input(**data, **bands)\n",
        "    latlon = torch.tensor([lat, lon])\n",
        "    months = create_month_tensor(start, end, delta)\n",
        "\n",
        "    return {\n",
        "        \"__key__\": f\"sample{index:6d}\",\n",
        "        \"inputs.pth\": x,\n",
        "        \"masks.pth\": mask,\n",
        "        \"dynamic_worlds.pth\": dw,\n",
        "        \"latlons.pth\": latlon,\n",
        "        \"months.pth\": months,\n",
        "        \"cls\": class_label,\n",
        "    }\n",
        "\n",
        "# TODO: add index property to each feature in col then map over the list of indices\n",
        "# using a filter to select the proper feature each time\n",
        "# this should avoid calls to getInfo, and toList along with having to recreate\n",
        "# the feature inside function each time\n",
        "# can pass the feature collection to the mapped function the same way that the\n",
        "# sink is passed\n",
        "def make_points_list(col):\n",
        "    points = trial_export_points.getInfo()['features']\n",
        "    class_labels = [x[\"properties\"][\"class\"] for x in points]\n",
        "    lons = [x[\"geometry\"][\"coordinates\"][0] for x in points]\n",
        "    lats = [x[\"geometry\"][\"coordinates\"][1] for x in points]\n",
        "    return zip(range(len(lats)), lats, lons, class_labels)\n",
        "\n",
        "points_list = make_points_list(trial_export_points)\n",
        "with Pool(MAX_PARALLEL_TASKS) as p:\n",
        "    outputs = p.map(process_point_fn, points_list)\n",
        "\n",
        "with wds.TarWriter(\"trial_dataset.tar\") as sink:\n",
        "    for output in outputs:\n",
        "        sink.write(output)"
      ],
      "metadata": {
        "id": "V3ybeNHGYHTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset = wds.WebDataset(\"trial_dataset.tar\")\n",
        "decoded_dataset = raw_dataset.decode().to_tuple(\n",
        "    \"cls\", \"inputs.pth\", \"masks.pth\", \"dynamic_worlds.pth\", \"latlons.pth\", \"months.pth\",\n",
        ")\n",
        "dataloader = torch.utils.data.DataLoader(decoded_dataset, batch_size=10, shuffle=False)\n",
        "# for i, elem in enumerate(decoded_dataset):\n",
        "    # if i >= 3:\n",
        "        # break\n",
        "    # print(i)\n",
        "    # for key, val in elem.items():\n",
        "        # print(key, val)"
      ],
      "metadata": {
        "id": "p1NNHnh9cLR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "pretrained_model = presto.Presto.load_pretrained()\n",
        "features_list = []\n",
        "class_list = []\n",
        "for (class_label, x, mask, dw, latlons, month) in tqdm(dataloader):\n",
        "    with torch.no_grad():\n",
        "        encodings = pretrained_model.encoder(\n",
        "            x, dynamic_world=dw, mask=mask, latlons=latlons, month=month\n",
        "        ).cpu().numpy()\n",
        "        features_list.append(encodings)\n",
        "        class_list.append(class_label)\n",
        "\n",
        "train_features = np.concatenate(features_list[:3])\n",
        "train_labels = np.concatenate(class_list[:3])\n",
        "test_features = np.concatenate(features_list[3:])\n",
        "test_labels = np.concatenate(class_list[3:])"
      ],
      "metadata": {
        "id": "aZZiF0UNghsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n",
        "model.fit(train_features, train_labels)\n",
        "predictions = np.argmax(model.predict_proba(test_features), 1)\n",
        "f1_score(test_labels, predictions)"
      ],
      "metadata": {
        "id": "FKbjhWNoqf_x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}