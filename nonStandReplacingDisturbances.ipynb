{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOV8kd9zHgP2eyVgzJ00P8y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boothmanrylan/nonStandReplacingDisturbances/blob/colab_dev/nonStandReplacingDisturbances.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "KVF8PINtvSkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/boothmanrylan/presto.git\n",
        "%cd presto\n",
        "\n",
        "# replace all install_requires '==' with '>=' to make installing to colab env easier\n",
        "setup_text = []\n",
        "with open(\"setup.py\", 'r') as f:\n",
        "    for line in f.readlines():\n",
        "        setup_text.append(line.replace(\"==\", \">=\"))\n",
        "\n",
        "with open(\"setup.py\", \"w\") as f:\n",
        "    for line in setup_text:\n",
        "        f.write(line)\n",
        "\n",
        "!pip install -e .\n",
        "%cd .."
      ],
      "metadata": {
        "id": "WJBh-pbRPPTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/boothmanrylan/nonStandReplacingDisturbances.git\n",
        "# %cd nonStandReplacingDisturbances"
      ],
      "metadata": {
        "id": "yUL8fDtjqqna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBMd-B1Ipxke"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import google\n",
        "from google.colab import auth\n",
        "import ee\n",
        "import geemap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "\n",
        "project = 'api-project-269347469410'\n",
        "asset_path = f\"projects/{project}/assets/rylan-nonstandreplacingdisturbances\"\n",
        "\n",
        "os.environ['GOOGLE_CLOUD_PROJECT'] = project\n",
        "!gcloud config set project {project}\n",
        "\n",
        "credentials, _ = google.auth.default()\n",
        "ee.Initialize(\n",
        "    credentials,\n",
        "    project=project,\n",
        "    opt_url='https://earthengine-highvolume.googleapis.com',\n",
        ")"
      ],
      "metadata": {
        "id": "PVgiQAoQqBvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get points"
      ],
      "metadata": {
        "id": "loMe5zIpvWgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "disturbed_regions = ee.FeatureCollection(f\"{asset_path}/my-data/usfs-nsr-disturbances\")\n",
        "\n",
        "# some of the regions had disturbances that were patchy over a large area\n",
        "# I was lazy when drawing these polygons so hold them out as regions to test\n",
        "# the model being applied over a large area\n",
        "largest_disturbed_regions = (\n",
        "    disturbed_regions\n",
        "    .map(lambda x: x.set('area', x.area(100)))\n",
        "    .limit(8, 'area', False)  # holding out 8 creates 5 regions after buffer/dissolve\n",
        ").geometry(100)\n",
        "\n",
        "buffered_disturbed_regions = disturbed_regions.map(\n",
        "    lambda x: x.buffer(1000, 100).bounds(100)\n",
        ")\n",
        "buffered_geometry = buffered_disturbed_regions.geometry(100).dissolve(100)\n",
        "\n",
        "def split_multipolygon(multipolygon):\n",
        "    # based on: https://gis.stackexchange.com/a/444779\n",
        "    size = multipolygon.coordinates().size()\n",
        "    indices = ee.List.sequence(0, size.subtract(1))\n",
        "\n",
        "    def grab_polygon(i):\n",
        "        geom = ee.Geometry.Polygon(multipolygon.coordinates().get(i))\n",
        "        return ee.Feature(geom, {'id': i, 'area': geom.area(100)})\n",
        "\n",
        "    return ee.FeatureCollection(indices.map(grab_polygon))\n",
        "\n",
        "split_geometry = split_multipolygon(buffered_geometry)\n",
        "largest_regions = split_geometry.filterBounds(largest_disturbed_regions)\n",
        "split_geometry = split_geometry.filter(ee.Filter.bounds(largest_disturbed_regions).Not())\n",
        "\n",
        "split_geometry = split_geometry.randomColumn('random', 42)\n",
        "train_regions = split_geometry.filter(ee.Filter.lte('random', 0.333))\n",
        "test_regions = split_geometry.filter(ee.Filter.And(\n",
        "    ee.Filter.gt('random', 0.333),\n",
        "    ee.Filter.lte('random', 0.666),\n",
        "))\n",
        "val_regions = split_geometry.filter(ee.Filter.gt('random', 0.666))\n",
        "\n",
        "def sample_points(rois):\n",
        "    disturbed_polys = disturbed_regions.filterBounds(rois)\n",
        "\n",
        "    # ensure each polygon has at least three samples in it\n",
        "    specific_disturbed_points = ee.FeatureCollection(disturbed_polys.map(\n",
        "        lambda x: ee.FeatureCollection.randomPoints(x.geometry(), 3, 42)\n",
        "    )).flatten()\n",
        "\n",
        "    # ensure that larger polygons have more samples in them\n",
        "    N = specific_disturbed_points.size()\n",
        "    other_disturbed_points = ee.FeatureCollection.randomPoints(\n",
        "        disturbed_polys.geometry(),\n",
        "        N.divide(10).int(),\n",
        "        42\n",
        "    )\n",
        "    disturbed_points = specific_disturbed_points.merge(other_disturbed_points)\n",
        "    disturbed_points = disturbed_points.map(lambda x: x.set('class', 1))\n",
        "\n",
        "    # ensure that there is the same number of disturbed as undisturbed samples\n",
        "    undisturbed_points = ee.FeatureCollection.randomPoints(\n",
        "        rois.geometry().difference(disturbed_polys),\n",
        "        disturbed_points.size(),\n",
        "        42,\n",
        "    ).map(lambda x: x.set('class', 0))\n",
        "\n",
        "    return disturbed_points.merge(undisturbed_points)\n",
        "\n",
        "train_points = sample_points(train_regions)\n",
        "test_points = sample_points(test_regions)\n",
        "val_points = sample_points(val_regions)\n",
        "\n",
        "print(train_points.size().getInfo(), test_points.size().getInfo(), val_points.size().getInfo())"
      ],
      "metadata": {
        "id": "o9MJs8AnqWpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Map = geemap.Map()\n",
        "Map.addLayer(disturbed_regions, {}, 'Disturbed Regions')\n",
        "Map.addLayer(largest_regions, {\"color\": \"purple\"}, 'Largest Disturbed Regions')\n",
        "Map.addLayer(split_geometry, {'color': 'white'}, 'Remaining Disturbed Regions')\n",
        "Map.addLayer(train_points, {'color': 'red'}, 'Train Points')\n",
        "Map.addLayer(test_points, {'color': 'blue'}, 'Test Points')\n",
        "Map.addLayer(val_points, {'color': 'yellow'}, 'Val Points')\n",
        "Map"
      ],
      "metadata": {
        "id": "zpVLbxdzsXJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Presto Inputs"
      ],
      "metadata": {
        "id": "h1LSOmO5vb83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_time_chunk_list(\n",
        "    start,\n",
        "    end,\n",
        "    delta=1,\n",
        "    exclude_months=None,\n",
        "    max_chunks=24,\n",
        "    extend_past_end=False,\n",
        "):\n",
        "    \"\"\" Create a list of start, stop pairs that can be used in filterDate\n",
        "\n",
        "    Regardless of what day of the month is given for start, start is\n",
        "    always set to the first of the given month. End is always set to the\n",
        "    first of the following month unless end is already the first of a mont\n",
        "    (end is exclusive in all ee filterDate methods). E.g.,\n",
        "    \"2021-01-15\"-\"2021-06-30\" would treated as \"2021-01-01\"-\"2021-07-01\".\n",
        "\n",
        "    Args:\n",
        "        start: string, start date in format YYYY-MM-dd.\n",
        "        end: string, end date in format YYYY-MM-dd.\n",
        "        delta: int, length (in months) for each time chunk.\n",
        "        exclude_months, List[int], exclude all time chunks that contain any\n",
        "            months in this list. Uses the ee convention of months being 1-12.\n",
        "        max_chunks: int, maximum number of time chunks to return. If set and the\n",
        "            number of time chunks created is greater than max_chunks, earlier\n",
        "            time chunks will be dropped so that the number of time chunks\n",
        "            returned is equal to max_chunks.\n",
        "        extend_past_end: bool, if True always the final time chunk to extend\n",
        "            past the end date.\n",
        "\n",
        "    Returns:\n",
        "        ee.List[ee.List[ee.Date]], start, stop date pairs\n",
        "    \"\"\"\n",
        "    start = ee.Date(start)\n",
        "    start = ee.Date.fromYMD(start.get(\"year\"), start.get(\"month\"), 1)\n",
        "\n",
        "    end = ee.Date(end)\n",
        "    if end.get(\"day\").getInfo() != 1:\n",
        "        end = ee.Date.fromYMD(end.get(\"year\"), end.get(\"month\").add(1), 1)\n",
        "\n",
        "    if extend_past_end:\n",
        "        total_months = end.difference(start, \"month\").ceil()\n",
        "        starts = ee.List.sequence(0, total_months.add(delta), delta)\n",
        "    else:\n",
        "        total_months = end.difference(start, \"month\").floor()\n",
        "        starts = ee.List.sequence(0, total_months, delta)\n",
        "\n",
        "    starts = starts.map(lambda x: start.advance(x, \"month\"))\n",
        "    ends = starts.map(lambda x: ee.Date(x).advance(delta, \"month\"))\n",
        "    chunks = starts.zip(ends)\n",
        "\n",
        "    if exclude_months is None:\n",
        "        return chunks\n",
        "\n",
        "    # earth engine doesnt allow filtering on an arbitrary function that returns\n",
        "    # a bool, so create a hacky feature that has a property we can filter on\n",
        "    def convert_to_features(startstop):\n",
        "        startstop = ee.List(startstop)\n",
        "        start = ee.Date(startstop.get(0))\n",
        "        advances = ee.List.sequence(0, delta)\n",
        "        months = advances.map(lambda x: start.advance(x, \"month\").get(\"month\"))\n",
        "        return ee.Feature(None, {\"startstop\": startstop, \"months\": months})\n",
        "\n",
        "    chunks = ee.FeatureCollection(chunks.map(convert_to_features))\n",
        "\n",
        "    filters = [\n",
        "        ee.Filter.listContains(leftField=\"months\", rightValue=x)\n",
        "        for x in exclude_months\n",
        "    ]\n",
        "    valid_month_filter = ee.Filter.Or(*filters).Not()\n",
        "    valid_chunks = chunks.filter(valid_month_filter)\n",
        "    chunks = valid_chunks.aggregate_array(\"startstop\")\n",
        "\n",
        "    if max_chunks is None:\n",
        "        return chunks\n",
        "\n",
        "    offset = chunks.length().subtract(max_chunks)\n",
        "    return chunks.slice(offset)"
      ],
      "metadata": {
        "id": "DNIZiM-DItvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from multiprocessing import Pool\n",
        "\n",
        "import numpy as np\n",
        "from numpy.lib.recfunctions import structured_to_unstructured\n",
        "from google.api_core import retry\n",
        "import webdataset as wds\n",
        "import torch\n",
        "from presto import presto\n",
        "\n",
        "SCALE = 10\n",
        "CRS = \"EPSG:3857\"\n",
        "PROJECTION = ee.Projection(CRS)\n",
        "MILLIS_PER_MONTH = 30 * 24 * 60 * 60 * 1000\n",
        "MAX_PARALLEL_TASKS = 40\n",
        "S2_BANDS = [\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\",\n",
        "            \"B8\", \"B8A\", \"B10\", \"B11\", \"B12\"]\n",
        "S1_BANDS = [\"VV\", \"VH\"]\n",
        "ERA5_BANDS = [\"temperature_2m\", \"total_precipitation\"]\n",
        "SRTM_BANDS = [\"elevation\", \"slope\"]\n",
        "DYNAMIC_WORLD_BANDS = [\"label\"]"
      ],
      "metadata": {
        "id": "4Qy9vxw0knG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_s1(point, time_chunks):\n",
        "    \"\"\" Creats a median Sentinel1 image for each period in time_chunks.\n",
        "\n",
        "    Based on:\n",
        "    openmapflow/openmapflow/eo/sentinel1.py:get_image_collection\n",
        "    and\n",
        "    openmapflow/openmapflow/eo/sentinel1.py:get_single_image\n",
        "    from: https://github.com/nasaharvest/openmapflow/tree/main\n",
        "\n",
        "    Args:\n",
        "        point: ee.Geometry, used to filterBounds of the complete sentinel1\n",
        "        time_chunks: ee.List[ee.List[ee.Date]], list of start stop date pairs.\n",
        "            Can be generate with create_time_chunk_list\n",
        "\n",
        "    Returns:\n",
        "        ee.Image\n",
        "    \"\"\"\n",
        "    true_start = ee.Date(ee.List(time_chunks.get(0)).get(0))\n",
        "    true_end = ee.Date(ee.List(time_chunks.get(-1)).get(1))\n",
        "    col = (\n",
        "        ee.ImageCollection(\"COPERNICUS/S1_GRD\")\n",
        "        .filterDate(true_start, true_end)\n",
        "        .filterBounds(point)\n",
        "        .filter(ee.Filter.eq(\"instrumentMode\", \"IW\"))\n",
        "        .filter(ee.Filter.listContains(\"transmitterReceiverPolarisation\", \"VV\"))\n",
        "        .filter(ee.Filter.listContains(\"transmitterReceiverPolarisation\", \"VH\"))\n",
        "    )\n",
        "\n",
        "    # want all images to either be descending or ascending, but not both. also\n",
        "    # want as many observations as possible, so take the collection that has the\n",
        "    # most images in it\n",
        "    descend_col = col.filter(ee.Filter.eq(\"orbitProperties_pass\", \"DESCENDING\"))\n",
        "    ascend_col = col.filter(ee.Filter.eq(\"orbitProperties_pass\", \"ASCENDING\"))\n",
        "    col = ee.ImageCollection(ee.Algorithms.If(\n",
        "        descend_col.size().gt(ascend_col.size()),\n",
        "        descend_col,\n",
        "        ascend_col,\n",
        "    ))\n",
        "\n",
        "    def process_time_chunk(chunk):\n",
        "        chunk = ee.List(chunk)\n",
        "        start = ee.Date(chunk.get(0))\n",
        "        stop = ee.Date(chunk.get(1))\n",
        "        curr_chunk = col.filterDate(start, stop).median()\n",
        "        # in case there are no images in chunk back fill with previous 3 months median\n",
        "        # prev = col.filterDate(start.advance(-6, \"month\"), stop).median()\n",
        "        # backfilled_chunk = ee.ImageCollection([curr_chunk, prev]).mosaic().select(S1_BANDS)\n",
        "        # return backfilled_chunk.toLong()\n",
        "        return curr_chunk.select(S1_BANDS)\n",
        "\n",
        "    return ee.ImageCollection(time_chunks.map(process_time_chunk)).toBands(), S1_BANDS"
      ],
      "metadata": {
        "id": "GM5AdO9yk5yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_s2(point, time_chunks):\n",
        "    \"\"\" Gets a quality mosaic Sentinel2 image for each period in time_chunks.\n",
        "\n",
        "    Based on:\n",
        "    openmapflow/openmapflow/eo/sentinel1.py:get_single_image\n",
        "    from: https://github.com/nasaharvest/openmapflow/tree/main\n",
        "\n",
        "    Args:\n",
        "        point: ee.Geometry, used to filterBounds\n",
        "        time_chunks: ee.List[ee.List[ee.Date]], list of start stop date pairs.\n",
        "            Can be generated with create_time_chunk_list\n",
        "\n",
        "    Returns:\n",
        "        ee.Image\n",
        "    \"\"\"\n",
        "    col = (\n",
        "        ee.ImageCollection(\"COPERNICUS/S2\")\n",
        "        .filterBounds(point)\n",
        "    )\n",
        "\n",
        "    def process_time_chunk(chunk):\n",
        "        chunk = ee.List(chunk)\n",
        "        cloud_score_plus = ee.ImageCollection(\"GOOGLE/CLOUD_SCORE_PLUS/V1/S2_HARMONIZED\")\n",
        "        curr_chunk = (\n",
        "            col.filterDate(chunk.get(0), chunk.get(1))\n",
        "            .linkCollection(cloud_score_plus, [\"cs_cdf\"])\n",
        "        )\n",
        "        # TODO: is a qualityMosaic better than masking and taking the median?\n",
        "        return curr_chunk.qualityMosaic(\"cs_cdf\").select(S2_BANDS).toLong()\n",
        "\n",
        "    return ee.ImageCollection(time_chunks.map(process_time_chunk)).toBands(), S2_BANDS"
      ],
      "metadata": {
        "id": "7SqhAs8Ek7DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_era5(point, time_chunks):\n",
        "    \"\"\" Gets a mean ERA5 image for each period in time_chunks.\n",
        "\n",
        "    All ERA5 images are dated to the first of the month, we choose to use\n",
        "    the ERA5 image based on the start date of each delta period.\n",
        "\n",
        "    Only returns the ERA5 temperature_2m and total_precipitation_sum bands.\n",
        "\n",
        "    Based on:\n",
        "    openmapflow/openmapflow/eo/era5.py:get_single_image\n",
        "    from: https://github.com/nasaharvest/openmapflow/tree/main\n",
        "\n",
        "    Args:\n",
        "        point: ee.Geometry, used to filterBounds\n",
        "        time_chunks: ee.List[ee.List[ee.Date]], list of start stop date pairs.\n",
        "            Can be generated with create_time_chunk_list\n",
        "\n",
        "    Returns:\n",
        "        ee.Image\n",
        "    \"\"\"\n",
        "    col = (\n",
        "        ee.ImageCollection(\"ECMWF/ERA5_LAND/MONTHLY_AGGR\")\n",
        "        .filterBounds(point)\n",
        "    )\n",
        "\n",
        "    def process_time_chunk(chunk):\n",
        "        start = ee.Date(ee.List(chunk).get(0)).advance(-1, \"day\")\n",
        "        end = ee.Date(ee.List(chunk).get(1))\n",
        "\n",
        "        curr_chunk = col.filterDate(start, end).mean()\n",
        "\n",
        "        temp = curr_chunk.select(\"temperature_2m\")\n",
        "        percip = curr_chunk.select(\"total_precipitation_sum\")\n",
        "        return ee.Image.cat([temp, percip]).toLong()\n",
        "\n",
        "    return ee.ImageCollection(time_chunks.map(process_time_chunk)).toBands(), ERA5_BANDS"
      ],
      "metadata": {
        "id": "Zgz0lYdRk8cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_srtm(*args, **kwargs):\n",
        "    \"\"\" Gets the SRTM DEM with calculated slope.\n",
        "\n",
        "    The SRTM is a single image that covers the entire globe at a single point in\n",
        "    time, therefore no need to create an image for each time step.\n",
        "\n",
        "    Args:\n",
        "        *args, **kwargs to allow for consistent usage with other data getters\n",
        "\n",
        "    Returns:\n",
        "        ee.Image\n",
        "    \"\"\"\n",
        "    elevation = ee.Image(\"USGS/SRTMGL1_003\").rename(\"elevation\")\n",
        "    slope = ee.Terrain.slope(elevation).rename(\"slope\")\n",
        "    return ee.Image.cat([elevation, slope]).toLong(), SRTM_BANDS"
      ],
      "metadata": {
        "id": "GXegZ72FlVyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dynamic_world(point, time_chunks):\n",
        "    \"\"\" Gets a mode Dynamic World image for each delta between start and stop.\n",
        "\n",
        "    Based on:\n",
        "    presto/presto/dataops/pipelines/dynamicworld.py:DynamicWorldMonthly2020_2021\n",
        "    from: https://github.com/nasaharvest/presto\n",
        "\n",
        "    Args:\n",
        "        point: ee.Geometry, used to filterBounds\n",
        "        time_chunks: ee.List[ee.List[ee.Date]], list of start stop date pairs.\n",
        "            Can be generated with create_time_chunk_list\n",
        "\n",
        "    Returns:\n",
        "        ee.Image\n",
        "    \"\"\"\n",
        "    col = (\n",
        "        ee.ImageCollection(\"GOOGLE/DYNAMICWORLD/V1\")\n",
        "        .filterBounds(point)\n",
        "    )\n",
        "\n",
        "    def process_time_chunk(chunk):\n",
        "        chunk = ee.List(chunk)\n",
        "        start = ee.Date(chunk.get(0))\n",
        "        end = ee.Date(chunk.get(1))\n",
        "\n",
        "        # in case there is no data in the current chunk replace with the mode\n",
        "        # class label from the three months prior to the current chunk\n",
        "        previous_year_data = col.filterDate(start.advance(-3, \"month\"), end)\n",
        "        previous_year_mode = previous_year_data.mode().select(\"label\")\n",
        "\n",
        "        curr_chunk = (\n",
        "            col.filterDate(start, end)\n",
        "            .mode()\n",
        "            .select(\"label\")\n",
        "            .unmask(previous_year_mode)\n",
        "        )\n",
        "\n",
        "        # in case there is also no data in the previous three months replace with 9\n",
        "        curr_chunk = curr_chunk.unmask(9)\n",
        "\n",
        "        return curr_chunk\n",
        "\n",
        "    return ee.ImageCollection(time_chunks.map(process_time_chunk)).toBands(), DYNAMIC_WORLD_BANDS"
      ],
      "metadata": {
        "id": "X6vSDRnylhCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiprocessing export"
      ],
      "metadata": {
        "id": "H3kiyOchvsiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_month_tensor(time_chunks):\n",
        "    # subtract 1 b/c Presto expects months to 0 - 11 but ee return 1 - 12\n",
        "    start_months = time_chunks.map(\n",
        "        lambda x: ee.Date(ee.List(x).get(0)).get(\"month\").subtract(1)\n",
        "    )\n",
        "    return torch.as_tensor(start_months.getInfo())\n",
        "\n",
        "\n",
        "@retry.Retry()\n",
        "def process_point_fn(\n",
        "    index,\n",
        "    point_list,\n",
        "    time_chunks,\n",
        "    crs=CRS,\n",
        "    scale=SCALE,\n",
        "):\n",
        "    \"\"\" Creates Presto input for given point.\n",
        "\n",
        "    Uses ee.data.computePixels to fetch data from Earth Engine.\n",
        "\n",
        "    Can be used as the funtion in multiprocessing.Pool.map to make GEE requests\n",
        "    in parallel.\n",
        "\n",
        "    Args:\n",
        "        index: int, index of current point to process\n",
        "        point_list: ee.List, all points to process\n",
        "        time_chunks: ee.List[ee.List[Date]], list of start stop date pairs\n",
        "        start: string, start date in format YYYY-MM-dd\n",
        "        end: string, end date in format YYYY-MM-dd\n",
        "        delta: int, chunk length to split total time period into (in ms).\n",
        "        crs: string, EPSG crs code defining the projection to get the data in.\n",
        "        scale: int, scale to get the data in.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    point = ee.Feature(point_list.get(index))\n",
        "    lon, lat = point.geometry().getInfo()[\"coordinates\"]\n",
        "\n",
        "    # project the point to the *unscaled* projection\n",
        "    projection = ee.Projection(crs)\n",
        "    coords = point.geometry(1, projection).getInfo()[\"coordinates\"]\n",
        "\n",
        "    request = {\n",
        "        \"fileFormat\": \"NUMPY_NDARRAY\",\n",
        "        \"grid\": {\n",
        "            \"dimensions\": {\n",
        "                \"width\": 1,\n",
        "                \"height\": 1,\n",
        "            },\n",
        "            \"affineTransform\": {\n",
        "                \"scaleX\": scale,\n",
        "                \"shearX\": 0,\n",
        "                \"translateX\": coords[0],\n",
        "                \"shearY\": 0,\n",
        "                \"scaleY\": -scale,\n",
        "                \"translateY\": coords[1],\n",
        "            },\n",
        "            \"crsCode\": crs\n",
        "        }\n",
        "    }\n",
        "\n",
        "    data = {}\n",
        "    bands = {}\n",
        "    data_getters = {\n",
        "        \"s1\": get_s1,\n",
        "        \"s2\": get_s2,\n",
        "        \"era5\": get_era5,\n",
        "        \"srtm\": get_srtm,\n",
        "        \"dynamic_world\": get_dynamic_world,\n",
        "    }\n",
        "    for source, get_image_fn in data_getters.items():\n",
        "        image, band_names = get_image_fn(point.geometry(), time_chunks)\n",
        "        request[\"expression\"] = image\n",
        "        raw = ee.data.computePixels(request)\n",
        "        arr = structured_to_unstructured(raw).reshape(-1, len(band_names))\n",
        "        arr = arr.astype(np.float32, copy=False)\n",
        "\n",
        "        if source == \"dynamic_world\":\n",
        "            data[source] = torch.as_tensor(arr.squeeze(), dtype=torch.int)\n",
        "        else:\n",
        "            bands[f\"{source}_bands\"] = band_names\n",
        "            data[source] = torch.as_tensor(arr, dtype=torch.float32)\n",
        "\n",
        "    # repeat srtm to be the same shape as all other inputs\n",
        "    num_timesteps = data[\"dynamic_world\"].shape[0]\n",
        "    data[\"srtm\"] = data[\"srtm\"].expand(num_timesteps, -1)\n",
        "\n",
        "    x, mask, dw = presto.construct_single_presto_input(**data, **bands)\n",
        "    latlon = torch.tensor([lat, lon])\n",
        "    months = create_month_tensor(time_chunks)\n",
        "\n",
        "    return {\n",
        "        \"__key__\": f\"sample{index:6d}\",\n",
        "        \"inputs.pth\": x,\n",
        "        \"masks.pth\": mask,\n",
        "        \"dynamic_worlds.pth\": dw,\n",
        "        \"latlons.pth\": latlon,\n",
        "        \"months.pth\": months,\n",
        "        \"cls\": point.getNumber('class').getInfo(),\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "V3ybeNHGYHTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col = train_points.randomColumn('random', 42).limit(50, 'random')\n",
        "chunks = create_time_chunk_list(\"2019-01-01\", \"2024-04-01\", 1, [10, 11, 12, 1, 2, 3], 24)\n",
        "output_path = \"trial_dataset.tar\"\n",
        "\n",
        "N = col.size().getInfo()\n",
        "indices = list(range(N))\n",
        "\n",
        "# create named function because multiprocessing requires the function to be\n",
        "# pickleable and lambda functions are not\n",
        "def fn(index):\n",
        "    return process_point_fn(index, col.toList(N), chunks)\n",
        "\n",
        "with Pool(MAX_PARALLEL_TASKS) as p:\n",
        "    outputs = p.map(fn, indices)\n",
        "\n",
        "with wds.TarWriter(output_path) as sink:\n",
        "    for output in outputs:\n",
        "        sink.write(output)"
      ],
      "metadata": {
        "id": "TtiEfTlXMMa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run downstream task"
      ],
      "metadata": {
        "id": "dwklHswSvljD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset = wds.WebDataset(\"trial_dataset.tar\")\n",
        "decoded_dataset = raw_dataset.decode().to_tuple(\n",
        "    \"cls\", \"inputs.pth\", \"masks.pth\", \"dynamic_worlds.pth\", \"latlons.pth\", \"months.pth\",\n",
        ")\n",
        "# dataloader = torch.utils.data.DataLoader(decoded_dataset, batch_size=10, shuffle=False)"
      ],
      "metadata": {
        "id": "p1NNHnh9cLR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "pretrained_model = presto.Presto.load_pretrained()\n",
        "features_list = []\n",
        "class_list = []\n",
        "for (class_label, x, mask, dw, latlons, month) in tqdm(dataloader):\n",
        "    with torch.no_grad():\n",
        "        encodings = pretrained_model.encoder(\n",
        "            x, dynamic_world=dw, mask=mask, latlons=latlons, month=month\n",
        "        ).cpu().numpy()\n",
        "        features_list.append(encodings)\n",
        "        class_list.append(class_label)\n",
        "\n",
        "train_features = np.concatenate(features_list[:3])\n",
        "train_labels = np.concatenate(class_list[:3])\n",
        "test_features = np.concatenate(features_list[3:])\n",
        "test_labels = np.concatenate(class_list[3:])"
      ],
      "metadata": {
        "id": "aZZiF0UNghsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n",
        "model.fit(train_features, train_labels)\n",
        "predictions = np.argmax(model.predict_proba(test_features), 1)\n",
        "f1_score(test_labels, predictions)"
      ],
      "metadata": {
        "id": "FKbjhWNoqf_x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}